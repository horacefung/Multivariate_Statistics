train <- read_csv('./Dataset/train.csv')
test <- read_csv('./Dataset/test.csv')
train$ln_income <- log(train$income)
train$ln_open_acc <- log(train$open_acc)
train$ln_loan_amnt <- log(train$loan_amnt)
test$ln_income <- log(test$income)
test$ln_open_acc <- log(test$open_acc)
test$ln_loan_amnt <- log(test$loan_amnt)
initial_fit <- glm(default ~ ln_income + ln_open_acc + ln_loan_amnt + dti + homeowner + employment
+ pub_bankrupt + pub_derog + delinq_2yrs,data=train, family=binomial, maxit=500, x=T)
summary(initial_fit)
gstat <- initial_fit$null.deviance - deviance(initial_fit)
cbind(gstat, 1-pchisq(gstat,length(coef(initial_fit))-1))
exp(coef(initial_fit)[-1])
drop1(initial_fit, test="LRT") #this is the deviance table
hoslem.test(train$default, fitted(initial_fit))
#Dxy (Somers D) is far too low
dd <- datadist(train)
options(datadist="dd")
initial_fit.lrm <- lrm(default ~ ln_income + ln_open_acc + ln_loan_amnt + dti + homeowner + employment
+ pub_bankrupt + pub_derog + delinq_2yrs, data=train, x=T, y=T)
residuals(initial_fit.lrm, type="gof") #global test of goodness of fit
initial_fit.lrm$stats
initial_diag <- glm.diag(initial_fit)
spearson <- residuals(initial_fit, type="pearson")/sqrt(1-initial_diag$h)
print.data.frame(cbind(spearson,initial_diag$cook,initial_diag$h))
glm.diag.plots(initial_fit, initial_diag)
train$row_id <- seq.int(nrow(train))
#2.5*(1+9)/1000 = 0.025
hatvalues <- data.frame(initial_diag$h)
hatvalues <- data.frame(tibble::rowid_to_column(hatvalues, "ID"))
hatvalues <- hatvalues[order(hatvalues$initial_diag.h, decreasing=TRUE),]
print.data.frame(hatvalues)
train[train$row_id %in% c(as.numeric(rownames(head(hatvalues, 28)))),]
cook <- data.frame(initial_diag$cook)
cook <- data.frame(tibble::rowid_to_column(cook, "ID"))
print.data.frame(cook[order(cook$initial_diag.cook, decreasing=TRUE),])
train[train$row_id %in% c(as.numeric(rownames(head(cook, 1)))),]
train2 <- train[!train$row_id %in% c(as.numeric(rownames(head(hatvalues, 28)))),]
train2 <- train2[!train2$row_id %in% c(as.numeric(rownames(head(cook, 1)))),]
train2$row_id <- seq.int(nrow(train2)) #need to reindex
fit2 <- glm(default ~ ln_income + ln_open_acc + ln_loan_amnt + dti + homeowner + employment
+ pub_bankrupt + pub_derog + delinq_2yrs,data=train2, family=binomial, maxit=500, x=T)
summary(fit2)
gstat <- fit2$null.deviance - deviance(fit2)
cbind(gstat, 1-pchisq(gstat,length(coef(fit2))-1))
hoslem.test(train2$default, fitted(fit2))
#Dxy (Somers D) is far too low
dd <- datadist(train2)
options(datadist="dd")
fit2.lrm <- lrm(default ~ ln_income + ln_open_acc + ln_loan_amnt + dti + homeowner + employment
+ pub_bankrupt + pub_derog + delinq_2yrs, data=train2, x=T, y=T)
residuals(fit2.lrm, type="gof") #global test of goodness of fit
fit2_diag <- glm.diag(fit2)
spearson <- residuals(fit2, type="pearson")/sqrt(1-fit2_diag$h)
p_residuals <- data.frame(cbind(spearson,fit2_diag$cook,fit2_diag$h))
print.data.frame(p_residuals)
residuals <- data.frame(tibble::rowid_to_column(p_residuals[1], "ID"))
residuals <- data.frame(residuals[order(residuals$spearson, decreasing=TRUE),])
print.data.frame(residuals)
#2.5*(1+9)/971 = 0.026
hatvalues <- data.frame(fit2_diag$h)
hatvalues <- data.frame(tibble::rowid_to_column(hatvalues, "ID"))
hatvalues <- hatvalues[order(hatvalues$fit2_diag.h, decreasing=TRUE),]
print.data.frame(hatvalues)
train2[train2$row_id %in% c(as.numeric(rownames(head(hatvalues, 36)))),]
cook <- data.frame(fit2_diag$cook)
cook <- data.frame(tibble::rowid_to_column(cook, "ID"))
print.data.frame(cook[order(cook$fit2_diag.cook, decreasing=TRUE),])
train2[train2$row_id %in% c(as.numeric(rownames(head(cook, 0)))),]
train3 <- train2[!train2$row_id %in% c(as.numeric(rownames(head(hatvalues, 36)))),]
#train3 <- train3[!train3$row_id %in% c(as.numeric(rownames(head(cook, 5)))),]
train3$row_id <- seq.int(nrow(train3)) #need to reindex
fit3 <- glm(default ~ ln_income + ln_open_acc + ln_loan_amnt + dti + homeowner + employment
+ pub_bankrupt + delinq_2yrs,data=train3, family=binomial, maxit=500, x=T)
summary(fit3)
gstat <- fit3$null.deviance - deviance(fit3)
cbind(gstat, 1-pchisq(gstat,length(coef(fit3))-1))
hoslem.test(train3$default, fitted(fit3))
exp(coef(fit3)[-1])
drop1(fit3, test="LRT")
#Dxy (Somers D) is far too low
dd <- datadist(train3)
options(datadist="dd")
fit3.lrm <- lrm(default ~ ln_income + ln_open_acc + ln_loan_amnt + dti + homeowner + employment
+ pub_bankrupt + delinq_2yrs, data=train3, x=T, y=T)
residuals(fit3.lrm, type="gof") #global test of goodness of fit
fit3.lrm$stats
fit3_diag <- glm.diag(fit3)
spearson <- residuals(fit3, type="pearson")/sqrt(1-fit3_diag$h)
p_residuals <- data.frame(cbind(spearson,fit3_diag$cook,fit3_diag$h))
glm.diag.plots(fit3, fit3_diag)
#Clear global environment
ls()
remove(list = ls())
gc()
train <- read.csv('./Dataset/cleaned_train.csv')
test <- read.csv('./Dataset/transformed_test.csv')
attach(train)
logitbest <- bestglm(data.frame(cbind(ln_income, ln_loan_amnt, dti, homeowner, employment),default), IC="AIC")
#AIC = 2k - 2ln, it doesnt matter if its negative, more negative generally better
logitbest$Subsets
aicc <- data.frame(logitbest$Subsets[8])
aicc$predictors <- c(0, 1, 2, 3, 4, 5)
aicc <- transform(aicc, AICc = AIC + (2*(predictors+2)*(predictors+3)/(935-predictors-3)))
train <- read.csv('./Dataset/cleaned_train.csv')
leverage_points <- read.csv('./Dataset/feed_unusual_observations.csv')
library(car)
library(readxl)
library(ggplot2)
library(tidyverse)
library(bestglm)
library(leaps)
library(rms)
library(ResourceSelection)
library(boot)
train <- read.csv('./Dataset/cleaned_train.csv')
leverage_points <- read_excel('./Dataset/feed_unusual_observations.xlsx')
test <- read.csv('./Dataset/transformed_test.csv')
attach(train)
logitbest <- bestglm(data.frame(cbind(ln_income, ln_loan_amnt, dti, homeowner, employment),default), IC="AIC")
#AIC = 2k - 2ln, it doesnt matter if its negative, more negative generally better
logitbest$Subsets
#Lets try 3, 4 and 5 predictors
fit3 <- glm(default ~ ln_loan_amnt + dti + homeowner,data=train, family=binomial, maxit=500, x=T)
summary(fit3)
#Dxy (Somers D) is far too low
dd <- datadist(train)
options(datadist="dd")
fit3.lrm <- lrm(default ~ ln_loan_amnt + dti + homeowner, data=train, x=T, y=T)
residuals(fit3.lrm, type="gof") #global test of goodness of fit
fit3.predict <- as.numeric(fitted(fit3) > .5)
table(default, fit3.predict)
table(leverage_points$default, fit3.predict)
pred <- predict(fit3, leverage_points)
predict <- as.numeric(pred > .50)
table(leverage_points$default, predict)
table(leverage_points$default, fit3.predict)
pred <- predict(fit3, leverage_points)
predict <- as.numeric(pred > .50)
table(leverage_points$default, predict)
fit4 <- glm(default ~ ln_income + ln_loan_amnt + dti + homeowner,data=train, family=binomial, maxit=500, x=T)
summary(fit4)
#Dxy (Somers D) is far too low
dd <- datadist(train)
options(datadist="dd")
fit4.lrm <- lrm(default ~ ln_income + ln_loan_amnt + dti + homeowner, data=train, x=T, y=T)
residuals(fit4.lrm, type="gof") #global test of goodness of fit
pred <- predict(fit4, leverage_points)
predict <- as.numeric(pred > .50)
table(leverage_points$default, predict)
fit5 <- glm(default ~ ln_income + ln_loan_amnt + dti + homeowner + employment,data=train, family=binomial, maxit=500, x=T)
summary(fit5)
#Dxy (Somers D) is far too low
dd <- datadist(train)
options(datadist="dd")
fit5.lrm <- lrm(default ~ ln_income + ln_loan_amnt + dti + homeowner + employment, data=train, x=T, y=T)
residuals(fit5.lrm, type="gof") #global test of goodness of fit
pred <- predict(fit5, leverage_points)
predict <- as.numeric(pred > .50)
table(leverage_points$default, predict)
gstat <- deviance(fit5) - deviance(fit4)
cbind(gstat, 1-pchisq(gstat,1)) #the DF is the difference between the number of parameters in the two models
deviance(fit4)
deviance(fit5)
deviance(fit4)
1-pchisq(gstat,1)
1-pchisq(gstat,2)
1-pchisq(gstat,10)
gstat <- deviance(fit4) - deviance(fit5)
cbind(gstat, 1-pchisq(gstat,1)) #the DF is the difference between the number of parameters in the two models
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .5)
table_results <- table(test$default, newclass.predict)
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .5)
table_results <- table(test$default, newclass.predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .5)
table_results <- table(test$default, newclass.predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 87 + 485
Nn <- 113 + 315
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .20)
table_results <- table(test$default, predict)
table_results
train2 <- train[!train$row_id %in% c(as.numeric(rownames(head(hatvalues, 19)))),]
train2 <- train2[!train2$row_id %in% c(as.numeric(rownames(head(cook, 3)))),]
#Clear global environment
ls()
remove(list = ls())
gc()
train <- read.csv('./Dataset/cleaned_train.csv')
leverage_points <- read_excel('./Dataset/feed_unusual_observations.xlsx')
test <- read.csv('./Dataset/transformed_test.csv')
attach(train)
logitbest <- bestglm(data.frame(cbind(ln_income, ln_loan_amnt, dti, homeowner, employment),default), IC="AIC")
#AIC = 2k - 2ln, it doesnt matter if its negative, more negative generally better
logitbest$Subsets
#Lets try 3, 4 and 5 predictors
fit3 <- glm(default ~ ln_loan_amnt + dti + homeowner,data=train, family=binomial, maxit=500, x=T)
summary(fit3)
gstat <- fit3$null.deviance - deviance(fit3)
cbind(gstat, 1-pchisq(gstat,length(coef(fit3))-1))
exp(coef(fit3)[-1])
drop1(fit3, test="LRT") #this is the deviance table
hoslem.test(train$default, fitted(fit3))
#Dxy (Somers D) is far too low
dd <- datadist(train)
options(datadist="dd")
fit3.lrm <- lrm(default ~ ln_loan_amnt + dti + homeowner, data=train, x=T, y=T)
residuals(fit3.lrm, type="gof") #global test of goodness of fit
fit3.lrm$stats
fit3.predict <- as.numeric(fitted(fit3) > .5)
table(default, fit3.predict)
pred <- predict(fit3, leverage_points)
predict <- as.numeric(pred > .50)
table(leverage_points$default, predict)
fit4 <- glm(default ~ ln_income + ln_loan_amnt + dti + homeowner,data=train, family=binomial, maxit=500, x=T)
#Dxy (Somers D) is far too low
dd <- datadist(train)
options(datadist="dd")
fit4.lrm <- lrm(default ~ ln_income + ln_loan_amnt + dti + homeowner, data=train, x=T, y=T)
residuals(fit4.lrm, type="gof") #global test of goodness of fit
fit4_diag <- glm.diag(fit4)
spearson <- residuals(fit4, type="pearson")/sqrt(1-fit4_diag$h)
print.data.frame(cbind(spearson,fit4_diag$cook,fit4_diag$h))
glm.diag.plots(fit4, fit4_diag)
train$row_id <- seq.int(nrow(train))
p_residuals <- data.frame(cbind(spearson,fit4_diag$cook,fit4_diag$h))
residuals <- data.frame(tibble::rowid_to_column(p_residuals[1], "ID"))
residuals <- data.frame(residuals[order(residuals$spearson, decreasing=TRUE),])
print.data.frame(residuals)
#2.5*(1+4)/935 = 0.013
hatvalues <- data.frame(fit4_diag$h)
hatvalues <- data.frame(tibble::rowid_to_column(hatvalues, "ID"))
hatvalues <- hatvalues[order(hatvalues$fit4_diag.h, decreasing=TRUE),]
print.data.frame(hatvalues)
print.data.frame(train[train$row_id %in% c(as.numeric(rownames(head(hatvalues, 19)))),])
cook <- data.frame(fit4_diag$cook)
cook <- data.frame(tibble::rowid_to_column(cook, "ID"))
print.data.frame(cook[order(cook$fit4_diag, decreasing=TRUE),])
train2 <- train[!train$row_id %in% c(as.numeric(rownames(head(hatvalues, 19)))),]
train2 <- train2[!train2$row_id %in% c(as.numeric(rownames(head(cook, 3)))),]
train2$row_id <- seq.int(nrow(train2)) #need to reindex
train2 <- train2[train2$row_id != 1,] #manually remove that weird point
train2 <- train[!train$row_id %in% c(as.numeric(rownames(head(hatvalues, 19)))),]
train2 <- train2[!train2$row_id %in% c(as.numeric(rownames(head(cook, 3)))),]
train2$row_id <- seq.int(nrow(train2)) #need to reindex
train2 <- train2[train2$row_id != 1,] #manually remove that weird point
##Check again
fit4 <- glm(default ~ ln_income + ln_loan_amnt + dti + homeowner,data=train2, family=binomial, maxit=500, x=T)
fit4_diag <- glm.diag(fit4)
spearson <- residuals(fit4, type="pearson")/sqrt(1-fit4_diag$h)
print.data.frame(cbind(spearson,fit4_diag$cook,fit4_diag$h))
glm.diag.plots(fit4, fit4_diag)
attach(train2)
logitbest <- bestglm(data.frame(cbind(ln_income, ln_loan_amnt, dti, homeowner, employment),default), IC="AIC")
#AIC = 2k - 2ln, it doesnt matter if its negative, more negative generally better
logitbest$Subsets
fit4 <- glm(default ~ ln_income + ln_loan_amnt + dti + homeowner,data=train2, family=binomial, maxit=500, x=T)
summary(fit4)
gstat <- fit4$null.deviance - deviance(fit4)
cbind(gstat, 1-pchisq(gstat,length(coef(fit4))-1))
exp(coef(fit4)[-1])
drop1(fit4, test="LRT") #this is the deviance table
hoslem.test(train2$default, fitted(fit4))
#Dxy (Somers D) is far too low
dd <- datadist(train)
options(datadist="dd")
fit4.lrm <- lrm(default ~ ln_income + ln_loan_amnt + dti + homeowner, data=train2, x=T, y=T)
residuals(fit4.lrm, type="gof") #global test of goodness of fit
fit4.lrm$stats
fit4.predict <- as.numeric(fitted(fit4) > .5)
table(default, fit4.predict)
fit4_diag <- glm.diag(fit4)
spearson <- data.frame(residuals(fit4, type="pearson")/sqrt(1-fit4_diag$h))
print.data.frame(cbind(spearson,fit4_diag$cook,fit4_diag$h))
glm.diag.plots(fit4, fit4_diag)
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .5)
table_results <- table(test$default, newclass.predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 87 + 476
Nn <- 113 + 324
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .20)
table_results <- table(test$default, predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 87 + 476
Nn <- 113 + 324
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .50)
table_results <- table(test$default, predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 87 + 476
Nn <- 113 + 324
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .20)
table_results <- table(test$default, predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 87 + 476
Nn <- 113 + 324
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .30)
table_results <- table(test$default, predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 87 + 476
Nn <- 113 + 324
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .20)
table_results <- table(test$default, predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .2)
table_results <- table(test$default, newclass.predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .3)
table_results <- table(test$default, newclass.predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .5)
table_results <- table(test$default, newclass.predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .4)
table_results <- table(test$default, newclass.predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 139 + 649
Nn <- 61 + 151
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .40)
table_results <- table(test$default, predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .3)
table_results <- table(test$default, newclass.predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 171 + 760
Nn <- 29 + 40
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .40)
table_results <- table(test$default, predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 171 + 760
Nn <- 29 + 40
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .30)
table_results <- table(test$default, predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .2)
table_results <- table(test$default, newclass.predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 199 + 798
Nn <- 1 + 2
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .20)
table_results <- table(test$default, predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .5)
table_results <- table(test$default, newclass.predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 87 + 476
Nn <- 113 + 324
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .20)
table_results <- table(test$default, predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .25)
table_results <- table(test$default, newclass.predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 192 + 791
Nn <- 8 + 9
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .25)
table_results <- table(test$default, predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 192 + 791
Nn <- 8 + 9
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .10)
table_results <- table(test$default, predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 192 + 791
Nn <- 8 + 9
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .05)
table_results <- table(test$default, predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 192 + 791
Nn <- 8 + 9
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .50)
table_results <- table(test$default, predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 192 + 791
Nn <- 8 + 9
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .01)
table_results <- table(test$default, predict)
table_results
#Initial incorrect predictions
pred.new <- predict(fit4, test, type="response")
newclass.predict <- as.numeric(pred.new > .5)
table_results <- table(test$default, newclass.predict)
table_results
#We need to adjust B0 to the base rate now
# Yes is a 1
Ny <- 87 + 476
Nn <- 113 + 324
prosplogit <- predict(fit4, test) + log((.2*Nn)/(.8*Ny))
prospprob <- exp(prosplogit)/(1 + exp(prosplogit))
predict <- as.numeric(prospprob > .20)
table_results <- table(test$default, predict)
table_results
vif(fit4)
vif(fit5)
#Clear global environment
ls()
remove(list = ls())
gc()
train <- read.csv('./Dataset/cleaned_train.csv')
leverage_points <- read_excel('./Dataset/feed_unusual_observations.xlsx')
test <- read.csv('./Dataset/transformed_test.csv')
attach(train)
logitbest <- bestglm(data.frame(cbind(ln_income, ln_loan_amnt, dti, homeowner, employment),default), IC="AIC")
#AIC = 2k - 2ln, it doesnt matter if its negative, more negative generally better
logitbest$Subsets
aicc <- data.frame(logitbest$Subsets[8])
aicc$predictors <- c(0, 1, 2, 3, 4, 5)
aicc <- transform(aicc, AICc = AIC + (2*(predictors+2)*(predictors+3)/(935-predictors-3)))
#Lets try 3, 4 and 5 predictors
fit3 <- glm(default ~ ln_loan_amnt + dti + homeowner,data=train, family=binomial, maxit=500, x=T)
summary(fit3)
vif(fit3)
#Dxy (Somers D) is far too low
dd <- datadist(train)
options(datadist="dd")
fit3.lrm <- lrm(default ~ ln_loan_amnt + dti + homeowner, data=train, x=T, y=T)
residuals(fit3.lrm, type="gof") #global test of goodness of fit
fit4 <- glm(default ~ ln_income + ln_loan_amnt + dti + homeowner,data=train, family=binomial, maxit=500, x=T)
summary(fit4)
vif(fit4)
fit5 <- glm(default ~ ln_income + ln_loan_amnt + dti + homeowner + employment,data=train, family=binomial, maxit=500, x=T)
summary(fit5)
vif(fit5)
